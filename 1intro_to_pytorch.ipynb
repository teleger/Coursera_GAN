{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/) 是一个非常强大的机器学习框架. PyTorch的中心是 [tensors](https://pytorch.org/docs/stable/tensors.html), 将矩阵推广到更高的秩。张量的一个直观例子是有三个颜色通道的图像:宽64像素、高64像素的3通道(红、绿、蓝)图像是一个3×64×64张量。您可以通过在代码的顶部附近编写import torch以及所有其他import语句来访问PyTorch框架\n",
    "\n",
    "本指南将帮助您介绍PyTorch的功能，但不必过于担心记住它:必要时，作业将链接到相关文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为甚么选择PyTorch?\n",
    "一个值得问的重要问题是，PyTorch为什么要用在这门课上?(https://thegradient.pub/state-of- ml-frameworks-2019-pytoring-dominates-research-tensorflow-dominates-industry/)从今天机器学习框架的状态来看，有一个很大的故障。在某种程度上，如本文所强调的，PyTorch通常比其他框架更符合python风格，更容易调试，是机器学习研究中使用最多的语言，而且这种语言的使用数量在不断增加。虽然PyTorch的主要替代品Tensorflow尝试集成了许多PyTorch的特性，但Tensorflow的实现有一些内在的限制，本文将重点介绍。\n",
    "\n",
    "\n",
    "值得注意的是，虽然PyTorch的工业用途已经增长，但Tensorflow仍然(目前)在工业上有一点受欢迎。在实践中，PyTorch对研究具有吸引力的特点也对教育具有吸引力，机器学习研究和实践的大趋势使其成为更主动的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量性质\n",
    "从一个列表或数组中创建张量的一种方法是使用 `torch.Tensor`. 我们会用它来举例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = torch.Tensor(\n",
    "    [\n",
    "     [[1, 2], [3, 4]], \n",
    "     [[5, 6], [7, 8]], \n",
    "     [[9, 0], [1, 2]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]],\n",
       "\n",
       "        [[9., 0.],\n",
       "         [1., 2.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量性质: device设备(张量类型）\n",
    "\n",
    "要查看张量的设备，你只需要写 `example_tensor.device`. 要将一个张量移动到一个新的设备，你可以写 `new_tensor = example_tensor.to(device)`\n",
    "其中设备将是 `cpu` 或者 `cuda`.例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量性质: shape形状\n",
    "你可以通过使用example_tensorflow打印出张量的形状来得到每个维度的元素数量。shape，如果你用过numpy，你可能对它很熟悉。例如，这个张量是一个3×2×2张量，因为它有3个元素，每个元素都是2×2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape[0] = 3\n",
      "size(1) = 2\n",
      "Rank = 3\n",
      "Number of elements = 12\n"
     ]
    }
   ],
   "source": [
    "print(\"shape[0] =\", example_tensor.shape[0])\n",
    "print(\"size(1) =\", example_tensor.size(1))\n",
    "print(\"Rank =\", len(example_tensor.shape))\n",
    "print(\"Number of elements =\", example_tensor.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7036,  0.2640],\n",
       "        [ 0.4839,  1.2586]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,2,device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的索引\n",
    "\n",
    "\n",
    "与numpy一样，您可以访问一个张量的特定元素或元素的子集。要访问$n$-th元素，只需编写' example_tensor[n] '—在Python中，这些维数都是0索引的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，如果你想访问$i$-th例子的$j$-th维，你可以写' example_tensor[i, j] '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor[1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想从一个张量中获取一个Python标量值，您可以使用' example_scalator .item() '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_scalar = example_tensor[1, 1, 0]\n",
    "example_scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，可以使用' x[:， i] '为列的第i个元素建立索引。例如，如果你想要“example_tensor”中每个元素的左上方元素，也就是每个矩阵的“0,0”元素，你可以写:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor[:, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  初始化张量\n",
    "\n",
    "\n",
    "在PyTorch中有许多创建新张量的方法，但在本课程中，最重要的是:\n",
    "\n",
    "\n",
    "[' torch.ones_like '](https://pytorch.org/docs/master/generated/torch.ones_like.html):创建一个具有与' example_tensor'相同形状和类型的所有张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(example_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`torch.zeros_like`](https://pytorch.org/docs/master/generated/torch.zeros_like.html): 创建一个具有与'example_tensor'相同形状和类型的所有零张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(example_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[' torch.randn_like '](https://pytorch.org/docs/stable/generated/torch.randn_like.html):创建一个张量，每个元素都采样自一个[正常(或高斯)分布](https://en.wikipedia.org/wiki/Normal_distribution)，具有与' example_tensor'相同的形状和类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3331,  2.2878],\n",
       "         [-0.4967,  0.2603]],\n",
       "\n",
       "        [[-0.4539, -0.1819],\n",
       "         [-1.6580,  0.6100]],\n",
       "\n",
       "        [[ 0.8696,  1.3040],\n",
       "         [ 0.4130, -1.1860]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn_like(example_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时(虽然比你想象的要少)，你可能需要初始化一个张量，只知道形状和类型，没有一个张量作为' ones_like '或' randn_like '的参考。在这种情况下，你可以创建一个$2x2$张量，如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6552, -0.7472],\n",
       "        [-1.2598, -1.7012]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 2, device='cpu') # 或者，对于GPU张量，你可以使用device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  基本功能\n",
    "\n",
    "\n",
    "要使用PyTorch，您应该知道许多基本的函数——如果您熟悉numpy，那么所有常用的函数都存在于PyTorch中，通常名称相同。您可以通过简单地写入“c * example_tensor”来执行元素对标量$c$的乘法/除法，通过写入“example_tensor + c”来执行元素对标量$c$的加法/减法。\n",
    "\n",
    "\n",
    "注意，大多数操作在PyTorch中都没有到位，这意味着它们不会更改原始变量的数据(但是，如果您愿意，可以将相同的变量名重新赋给已更改的数据，例如' example_tensor= example_tensor+ 1 ')。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -8.,  -6.],\n",
       "         [ -4.,  -2.]],\n",
       "\n",
       "        [[  0.,   2.],\n",
       "         [  4.,   6.]],\n",
       "\n",
       "        [[  8., -10.],\n",
       "         [ -8.,  -6.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_tensor - 5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor(4.)\n",
      "Stdev: tensor(2.9848)\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\", example_tensor.mean())\n",
    "print(\"Stdev:\", example_tensor.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 神经网络模块 (`torch.nn`)\n",
    "\n",
    "PyTorch 有这十分强大的类 `torch.nn` 模块 (通常我们, 简称 `nn`). 这些类允许你创建一个新的类或是函数，它以特殊的方式变换一个张量，当多次调用她的时候保留信息（关键）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Linear`\n",
    "\n",
    "要创建线性层，您需要向它传递输入维度和输出维度的数量。初始化为' nn的线性对象。Linear(10,2) '将接受一个$n\\times10$矩阵，并返回一个$n\\times2$矩阵，其中所有$n$元素都执行了相同的线性变换。例如，您可以初始化一个执行操作$Ax + b$的线性层，其中，当您生成[' nn.Linear() '](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)对象时，将随机初始化$ a $和$b$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2023, -0.3197],\n",
       "        [-0.4027, -0.3584],\n",
       "        [-0.1957,  0.5386]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(10, 2)\n",
    "example_input = torch.randn(3, 10)\n",
    "example_output = linear(example_input)\n",
    "example_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.ReLU`\n",
    "\n",
    "\n",
    "[' nn.ReLU() '](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)将创建一个对象，当接收到一个张量时，该对象将执行一个ReLU激活函数。这将在课堂上进一步复习，但在本质上，ReLU非线性将一个张量中的所有负数都设为零。一般来说，最简单的神经网络由一系列线性变换组成，每个变换后都有激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.5386]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "relu_output = relu(example_output)\n",
    "relu_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `nn.BatchNorm1d`\n",
    "\n",
    "\n",
    "[' nn.BatchNorm1d '](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)是一种标准化技术，该技术将重新调整一批$n$的输入，使批之间具有一致的平均值和标准偏差。\n",
    "\n",
    "\n",
    "正如其名称中的“1d”所示，这适用于需要一组输入的情况，其中每个输入都是数字的平面列表。换句话说，每个输入都是一个向量，而不是矩阵或高维张量。对于一组图像，其中每个都是一个高维张量，您可以使用[' nn.BatchNorm2d '](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)，稍后将在此页上讨论。\n",
    "\n",
    "\n",
    "的神经网络。BatchNorm1d’接受一个参数，该参数表示批处理中每个对象的输入维数(每个示例向量的大小)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.7071],\n",
       "        [ 0.0000, -0.7071],\n",
       "        [ 0.0000,  1.4141]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm = nn.BatchNorm1d(2)\n",
    "batchnorm_output = batchnorm(relu_output)\n",
    "batchnorm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Sequential`\n",
    "\n",
    "[' nn.Sequential '](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)创建一个执行一系列操作的单个操作。例如，您可以编写一个批处理归一化为的神经网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "tensor([[ 0.6219, -0.3637,  0.9766,  0.7855,  0.0853],\n",
      "        [ 0.0526, -0.1271,  0.0208,  1.0326,  1.2976],\n",
      "        [ 2.0590,  4.0993,  0.4174,  1.3265,  0.7033],\n",
      "        [ 0.5153, -0.3124,  1.4581,  1.9335,  0.1559],\n",
      "        [ 0.1047, -0.1697, -1.2252, -0.0865, -0.0425]])\n",
      "output: \n",
      "tensor([[0.3213, 0.0000],\n",
      "        [0.3695, 0.4435],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [1.3112, 1.6621]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mlp_layer = nn.Sequential(\n",
    "    nn.Linear(5, 2),\n",
    "    nn.BatchNorm1d(2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "test_example = torch.randn(5,5) + 1\n",
    "print(\"input: \")\n",
    "print(test_example)\n",
    "print(\"output: \")\n",
    "print(mlp_layer(test_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 优化\n",
    "\n",
    "本质上任何机器学习框架最重要的方面之一就是它的自动区分库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "\n",
    "要在PyTorch中创建一个优化器，你需要使用“torch”。模块，通常导入为“optim”。[' optim.Adam '](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)对应于Adam优化器。要创建优化器对象，您需要向它传递要优化的参数和学习速率' lr '，以及特定于优化器的任何其他参数。\n",
    "\n",
    "对于所有' nn '对象，您可以使用它们的' parameters() '方法以列表的形式访问它们的参数，如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "adam_opt = optim.Adam(mlp_layer.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环训练\n",
    "\n",
    "PyTorch的一个(基本)训练步骤包括四个基本部分:\n",
    "\n",
    "\n",
    "* 使用`opt.zero_grad()`将所有的渐变设置为零\n",
    "\n",
    "* 计算`loss`\n",
    "\n",
    "* 使用`loss.backward()`计算关于loss的梯度\n",
    "\n",
    "* 使用`opt.step()`更新正在优化的参数\n",
    "\n",
    "这可能看起来像下面的代码(你会注意到，如果你运行几次，损失下降):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5299, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_example = torch.randn(100,5) + 1\n",
    "adam_opt.zero_grad()\n",
    "\n",
    "# We'll use a simple loss function of mean distance from 1\n",
    "# torch.abs takes the absolute value of a tensor\n",
    "cur_loss = torch.abs(1 - mlp_layer(train_example)).mean()\n",
    "\n",
    "cur_loss.backward()\n",
    "adam_opt.step()\n",
    "print(cur_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `requires_grad_()`\n",
    "\n",
    "你也可以告诉PyTorch它需要计算你创建的张量的梯度通过说\" example_tensor.requires_grad_() \"这将会改变它的位置。这意味着即使PyTorch通常不会为那个特定张量存储一个梯度，它会为那个特定张量存储梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `with torch.no_grad():`\n",
    "\n",
    "PyTorch通常在对张量进行一系列操作时计算梯度。这通常会占用不必要的计算和内存，特别是在执行计算时。但是，您可以用' with torch.no_grad() '包装一段代码，以防止在一段代码中计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `detach():`\n",
    "\n",
    "有时，你想计算和使用一个张量的值而不计算它的梯度。举个例子,如果你有两个模型,A和B,你想直接优化的参数B的输出,而不计算梯度通过B,然后你可以养活的分离输出B A。有很多原因你可能想这样做,包括效率或周期性的依赖关系(即取决于B)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New `nn` Classes\n",
    "\n",
    "您还可以创建扩展nn模块的新类。对于这些类，使用所有类属性，如self。如果参数本身是nn对象，或者是封装在nn中的张量，则参数将自动被视为参数。参数，该参数由类初始化。\n",
    "\n",
    "\n",
    "剩余的init__函数定义了创建对象时将要发生的事情。类的init函数的第一行，例如，WellNamedClass，需要是super(WellNamedClass, self).其他的init__()。\n",
    "\n",
    "\n",
    "前向函数定义了如果你创建对象模型并传递一个张量x，运行什么，就像模型(x)中那样。如果你选择函数签名,(自我,x),然后每个调用的函数,得到两条信息:自我,这是一个对象的引用,您可以访问所有的参数,和x,这是目前你想返回y的张量。\n",
    "\n",
    "\n",
    "一个类可能看起来如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModule(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super(ExampleModule, self).__init__()\n",
    "        self.linear = nn.Linear(input_dims, output_dims)\n",
    "        self.exponent = nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # This is the notation for element-wise exponentiation, \n",
    "        # which matches python in general\n",
    "        x = x ** self.exponent \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor(1., requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.3032,  0.2899, -0.0449,  0.1387,  0.1259,  0.0892, -0.2656,  0.2020,\n",
       "           0.0320,  0.1441],\n",
       "         [ 0.0158,  0.2713,  0.2985, -0.1731, -0.1460, -0.0075,  0.1108,  0.0236,\n",
       "           0.2062,  0.0778]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0544, 0.2264], requires_grad=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model = ExampleModule(10, 2)\n",
    "list(example_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exponent',\n",
       "  Parameter containing:\n",
       "  tensor(1., requires_grad=True)),\n",
       " ('linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.3032,  0.2899, -0.0449,  0.1387,  0.1259,  0.0892, -0.2656,  0.2020,\n",
       "            0.0320,  0.1441],\n",
       "          [ 0.0158,  0.2713,  0.2985, -0.1731, -0.1460, -0.0075,  0.1108,  0.0236,\n",
       "            0.2062,  0.0778]], requires_grad=True)),\n",
       " ('linear.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0544, 0.2264], requires_grad=True))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(example_model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0911, -0.1459],\n",
       "        [ 0.6616,  0.0207]], grad_fn=<PowBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2, 10)\n",
    "example_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D\n",
    "\n",
    "这节课不需要这些，这些背后的理论将在后面的课程中被更多地回顾，但这里有一个快速参考:\n",
    "\n",
    "\n",
    "二维旋转:神经网络。Conv2d需要输入和输出通道的数量以及内核的大小。\n",
    "\n",
    "2D转置卷积(又名反卷积):nn。ConvTranspose2d还需要输入和输出通道的数量以及内核的大小\n",
    "\n",
    "2D批处理归一化:nn。BatchNorm2d需要输入维数\n",
    "\n",
    "调整图片大小:神经网络。上样需要最终的尺寸或比例因子。另外，n.function .interpolate也采用相同的参数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
